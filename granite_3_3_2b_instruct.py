# -*- coding: utf-8 -*-
"""granite-3.3-2b-instruct.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//huggingface.co/ibm-granite/granite-3.3-2b-instruct.ipynb
"""

!pip install -U transformers

# ==============================
# StudyMate: PDF Q&A Assistant
# IBM Granite 3.3 2B Instruct + FAISS + Gradio
# Single-cell runnable script
# ==============================

!pip install transformers sentence-transformers faiss-cpu pymupdf gradio accelerate --quiet

import fitz  # PyMuPDF
import faiss
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from sentence_transformers import SentenceTransformer
import gradio as gr
import os

# ==============================
# Load Models
# ==============================

# Embedding model
embed_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# IBM Granite 3.3 2B Instruct Model
model_name = "ibm-granite/granite-3.3-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
llm_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

gen_pipeline = pipeline(
    "text-generation",
    model=llm_model,
    tokenizer=tokenizer,
    max_new_tokens=300,
    top_p=0.9,
    temperature=0.2,
)

# ==============================
# PDF Processing
# ==============================

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# Split into chunks
def chunk_text(text, chunk_size=500):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size):
        chunk = " ".join(words[i:i+chunk_size])
        chunks.append(chunk)
    return chunks

# ==============================
# FAISS Semantic Search
# ==============================

faiss_index = None
document_chunks = []

def build_faiss_index(chunks):
    global faiss_index, document_chunks
    document_chunks = chunks

    embeddings = embed_model.encode(chunks, convert_to_numpy=True)

    dim = embeddings.shape[1]
    faiss_index = faiss.IndexFlatL2(dim)
    faiss_index.add(embeddings)

def search(query, k=3):
    query_embed = embed_model.encode([query], convert_to_numpy=True)
    distances, indices = faiss_index.search(query_embed, k)
    results = []
    for idx in indices[0]:
        results.append(document_chunks[idx])
    return results

# ==============================
# LLM Response Generation
# ==============================

def answer_question(query):
    relevant_chunks = search(query, k=3)
    context = "\n\n".join(relevant_chunks)

    prompt = f"""
You are StudyMate, an academic assistant. Answer the user's question using ONLY the information from the provided context.
If the answer is not in the context, say "The answer is not found in the uploaded documents."

Context:
{context}

Question: {query}

Answer:
"""

    output = gen_pipeline(prompt)[0]["generated_text"]
    return output

# ==============================
# Gradio UI
# ==============================

def upload_pdfs(files):
    all_text = ""
    for f in files:
        text = extract_text_from_pdf(f.name)
        all_text += text + "\n"

    chunks = chunk_text(all_text)
    build_faiss_index(chunks)

    return "PDFs uploaded & indexed successfully! You can now ask questions."

def ask(query):
    return answer_question(query)

with gr.Blocks() as app:
    gr.Markdown("# üìò StudyMate ‚Äì AI Academic PDF Assistant\n### Powered by IBM Granite 3.3 2B Instruct + FAISS")

    pdf_upload = gr.Files(type="filepath", file_count="multiple")
    upload_btn = gr.Button("Process PDFs")
    status = gr.Textbox(label="Status")

    query = gr.Textbox(label="Ask a Question")
    ask_btn = gr.Button("Get Answer")
    output = gr.Textbox(label="Answer")

    upload_btn.click(upload_pdfs, inputs=pdf_upload, outputs=status)
    ask_btn.click(ask, inputs=query, outputs=output)

app.launch()

"""## Local Inference on GPU
Model page: https://huggingface.co/ibm-granite/granite-3.3-2b-instruct

‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/ibm-granite/granite-3.3-2b-instruct)
			and/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè
"""

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-generation", model="ibm-granite/granite-3.3-2b-instruct")
messages = [
    {"role": "user", "content": "Who are you?"},
]
pipe(messages)

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("ibm-granite/granite-3.3-2b-instruct")
model = AutoModelForCausalLM.from_pretrained("ibm-granite/granite-3.3-2b-instruct")
messages = [
    {"role": "user", "content": "Who are you?"},
]
inputs = tokenizer.apply_chat_template(
	messages,
	add_generation_prompt=True,
	tokenize=True,
	return_dict=True,
	return_tensors="pt",
).to(model.device)

outputs = model.generate(**inputs, max_new_tokens=40)
print(tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:]))